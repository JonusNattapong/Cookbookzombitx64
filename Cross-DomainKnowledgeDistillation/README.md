# Cross-Domain Knowledge Distillation (การถ่ายทอดความรู้ข้ามโดเมน)

## คำอธิบาย
Cross-Domain Knowledge Distillation เป็นเทคนิคในการถ่ายทอดความรู้จากโมเดล "ครู" (teacher) ที่ทำงานบนโดเมนหนึ่ง (เช่น ภาพ) ไปยังโมเดล "นักเรียน" (student) ที่ทำงานบนอีกโดเมนหนึ่ง (เช่น ข้อความ) ช่วยให้โมเดลที่อาจมีขนาดเล็กกว่าหรือทำงานกับข้อมูลต่างประเภทสามารถเรียนรู้ได้ดีขึ้น

## หลักการทำงาน
1. **ครู (Teacher)**: โมเดลที่มีประสิทธิภาพสูงฝึกสอนบนโดเมนต้นทาง (เช่น ภาพ)
2. **นักเรียน (Student)**: โมเดลที่เราต้องการพัฒนาให้ดีขึ้นบนโดเมนปลายทาง (เช่น ข้อความ)
3. **การถ่ายทอดความรู้**: ใช้ output จากโมเดลครูเพื่อแนะนำการเรียนรู้ของโมเดลนักเรียน ผ่านการใช้:
   - Feature distillation: ถ่ายทอดความรู้ระดับ feature
   - Soft targets: ถ่ายทอดผ่าน probability distribution
   - Intermediate representations: ถ่ายทอดความรู้จากชั้นกลางของโมเดล

## ข้อดี
- ช่วยปรับปรุงประสิทธิภาพของโมเดลบนโดเมนที่มีข้อมูลน้อย
- ถ่ายทอดความรู้ระหว่างรูปแบบข้อมูลที่แตกต่างกัน
- ลดความต้องการในการติดฉลากข้อมูล (labeled data) จำนวนมาก
- เหมาะสำหรับการทำ transfer learning ระหว่างโดเมนที่แตกต่างกัน

## ข้อจำกัด
- ต้องมีข้อมูลที่เป็น multi-modal (หลายรูปแบบ) สำหรับฝึกสอน
- การออกแบบวิธีการถ่ายทอดความรู้ที่เหมาะสมเป็นเรื่องท้าทาย
- อาจต้องการการปรับแต่งพารามิเตอร์เพิ่มเติม (α, temperature)
- ประสิทธิภาพขึ้นอยู่กับความเหมือนหรือความเกี่ยวข้องระหว่างโดเมน

## ตัวอย่างการใช้งาน
ไฟล์ `cross_domain_knowledge_distillation.py` แสดงตัวอย่างการใช้ Cross-Domain Knowledge Distillation ระหว่างโมเดลภาพ (ResNet18) และโมเดลข้อความ (LSTM)

### การทำงานหลัก:
1. ฝึกสอนโมเดลครู (teacher) ด้วยข้อมูลภาพ
2. ฝึกสอนโมเดลนักเรียน (student) ด้วยข้อมูลข้อความในสองรูปแบบ:
   - ใช้ Knowledge Distillation
   - ไม่ใช้ Knowledge Distillation
3. เปรียบเทียบประสิทธิภาพของทั้งสองโมเดล

### การแสดงผล:
- กราฟแสดงผลการฝึกสอน (loss และ accuracy)
- การเปรียบเทียบประสิทธิภาพระหว่างวิธีการต่างๆ
- สรุปผลการทดลองและประสิทธิภาพที่ดีขึ้น

## การนำไปใช้งานในอุตสาหกรรม
- การแปลงความรู้จากโมเดลภาพขนาดใหญ่ไปสู่โมเดลข้อความหรือเสียง
- แอปพลิเคชันมัลติโมดัล (multimodal) ที่ต้องการความเข้าใจข้ามโดเมน
- ระบบแนะนำที่ทำงานกับข้อมูลหลายประเภท
- การถ่ายทอดความรู้จากโมเดลขนาดใหญ่ไปสู่อุปกรณ์ที่มีข้อจำกัดด้านทรัพยากร

## อ้างอิง
- Gupta, S., Hoffman, J., & Malik, J. (2016). Cross modal distillation for supervision transfer.
- Tian, Y., Krishnan, D., & Isola, P. (2019). Contrastive representation distillation.
- Liu, Y., Jia, X., Tan, M., Vemulapalli, R., Zhu, Y., Green, B., & Wang, X. (2021). Search to distill: Pearls are everywhere but not the eyes. 