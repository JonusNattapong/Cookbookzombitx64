#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ตัวอย่างการใช้งานโมดูล LLM ORPO (Optimized Rank Preference Optimization)
"""

import os
import torch
from dotenv import load_dotenv
from cookbookzombitx64.llm.orpo import LLMORPOTrainer

# โหลดค่าตัวแปรสภาพแวดล้อม
load_dotenv()

def main():
    """ฟังก์ชันหลักสำหรับตัวอย่างการใช้งาน"""
    
    # ตั้งค่าโมเดลที่จะใช้
    model_name = "gpt2"  # ใช้โมเดล GPT-2 ขนาดเล็กเพื่อเป็นตัวอย่าง
    
    print(f"เริ่มต้นการใช้งาน LLM ORPO Trainer ด้วยโมเดล {model_name}")
    
    # สร้างออบเจ็กต์ LLMORPOTrainer
    trainer = LLMORPOTrainer(model_name, beta=0.1)
    
    print(f"โมเดลทำงานบนอุปกรณ์: {trainer.device}")
    
    # สร้างข้อมูลตัวอย่างสำหรับการฝึก ORPO
    prompts = [
        "คำถาม: ทำไมท้องฟ้าเป็นสีฟ้า?\nคำตอบ:",
        "คำถาม: อธิบายหลักการของทฤษฎีสัมพัทธภาพพิเศษ\nคำตอบ:",
        "คำถาม: วิธีทำน้ำแข็งใสที่บ้าน\nคำตอบ:"
    ]
    
    chosen_responses = [
        "ท้องฟ้าเป็นสีฟ้าเพราะแสงอาทิตย์กระทบกับอนุภาคในชั้นบรรยากาศและเกิดการกระเจิงของแสง โดยแสงสีฟ้าจะกระเจิงได้ดีกว่าแสงสีอื่นๆ ปรากฏการณ์นี้เรียกว่า การกระเจิงของเรย์ลี (Rayleigh scattering)",
        "ทฤษฎีสัมพัทธภาพพิเศษเสนอโดยไอน์สไตน์ในปี 1905 มีหลักการสำคัญคือ 1) กฎของฟิสิกส์เหมือนกันในทุกกรอบอ้างอิงที่เคลื่อนที่ด้วยความเร็วคงที่ 2) ความเร็วแสงคงที่ในทุกกรอบอ้างอิงไม่ว่าผู้สังเกตจะเคลื่อนที่อย่างไร ทฤษฎีนี้นำไปสู่สมการ E=mc² และปรากฏการณ์ต่างๆ เช่น การยืดของเวลาและการหดของความยาว",
        "วิธีทำน้ำแข็งใสที่บ้าน: 1) ต้มน้ำให้เดือดเพื่อไล่ก๊าซออกซิเจนที่ละลายในน้ำซึ่งเป็นสาเหตุของความขุ่น 2) ปล่อยให้น้ำเย็นลงและปิดฝา 3) แช่ในช่องแข็งโดยใช้ถาดที่มีฉนวนรอบข้างเพื่อให้น้ำแข็งแข็งตัวจากด้านบนลงล่าง 4) เมื่อแข็งตัวสมบูรณ์จะได้น้ำแข็งที่ใสกว่าการแช่แข็งน้ำปกติ"
    ]
    
    rejected_responses = [
        "ท้องฟ้าเป็นสีฟ้าเพราะมันสะท้อนสีของมหาสมุทร",
        "ทฤษฎีสัมพัทธภาพพิเศษคือทฤษฎีที่บอกว่าทุกอย่างเป็นเรื่องสัมพัทธ์และขึ้นอยู่กับมุมมองของแต่ละคน",
        "วิธีทำน้ำแข็งใสคือใส่น้ำในถาดน้ำแข็งแล้วแช่ตู้เย็นเหมือนปกติ"
    ]
    
    print("เตรียมชุดข้อมูลสำหรับ ORPO...")
    prepared_dataset = trainer.prepare_dataset(prompts, chosen_responses, rejected_responses, max_length=256)
    print(f"เตรียมชุดข้อมูลเสร็จสิ้น จำนวน {len(prepared_dataset)} ตัวอย่าง")
    
    # เทรนโมเดล (ใช้ค่าที่น้อยเพื่อเป็นตัวอย่าง)
    print("เริ่มการเทรนโมเดลด้วย ORPO...")
    trainer.train(
        dataset=prepared_dataset,
        output_dir="./results/llm_orpo",
        num_train_epochs=1,
        per_device_train_batch_size=1,
        learning_rate=1e-5,
        weight_decay=0.01,
        save_steps=10
    )
    
    # บันทึกโมเดล
    print("บันทึกโมเดล...")
    trainer.save_model("./models/llm_orpo_model")
    
    # ทดสอบการสร้างข้อความ
    test_prompts = [
        "คำถาม: อธิบายวิธีการเรียนรู้ที่มีประสิทธิภาพ\nคำตอบ:",
        "คำถาม: ทำไมการออกกำลังกายจึงมีประโยชน์\nคำตอบ:"
    ]
    
    print("\nทดสอบการสร้างข้อความจากโมเดลที่เทรนแล้ว:")
    for prompt in test_prompts:
        print(f"\nPrompt: {prompt}")
        generated_text = trainer.generate(prompt, max_length=150)
        print(f"คำตอบ: {generated_text[0]}")
    
    print("\nการทดสอบเสร็จสิ้น")

if __name__ == "__main__":
    main() 